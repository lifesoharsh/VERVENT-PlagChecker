<!DOCTYPE html>
<!-- saved from url=(0082)https://srome.github.io/Understanding-Attention-in-Neural-Networks-Mathematically/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Understanding Attention in Neural Networks Mathematically – Scott Rome – Math Ph.D. who works in Machine Learning</title>

        
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

    
    <meta name="description" content="Attention has gotten plenty of attention lately, after yielding state of the art results in multiple fields of research. From image captioning and language translation to interactive question answering, Attention has quickly become a key tool to which researchers must attend. Some have taken notice and even postulate that attention is all you need. But what is Attention anyway? Should you pay attention to Attention? Attention enables the model to focus in on important pieces of the feature space. In this post, we explain how the Attention mechanism works mathematically and then implement the equations using Keras. We conclude with discussing how to “see” the Attention mechanism at work by identifying important words for a classification task.

">
    <meta property="og:description" content="Attention has gotten plenty of attention lately, after yielding state of the art results in multiple fields of research. From image captioning and language translation to interactive question answering, Attention has quickly become a key tool to which researchers must attend. Some have taken notice and even postulate that attention is all you need. But what is Attention anyway? Should you pay attention to Attention? Attention enables the model to focus in on important pieces of the feature space. In this post, we explain how the Attention mechanism works mathematically and then implement the equations using Keras. We conclude with discussing how to “see” the Attention mechanism at work by identifying important words for a classification task.

">
    
    <meta name="author" content="Scott Rome">

    
    <meta property="og:title" content="Understanding Attention in Neural Networks Mathematically">
    <meta property="twitter:title" content="Understanding Attention in Neural Networks Mathematically">
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="ref_25_files/style.css">
    <link rel="alternate" type="application/rss+xml" title="Scott Rome - Math Ph.D. who works in Machine Learning" href="https://srome.github.io/feed.xml">


    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Understanding Attention in Neural Networks Mathematically | Scott Rome</title>
<meta name="generator" content="Jekyll v3.7.3.1">
<meta property="og:title" content="Understanding Attention in Neural Networks Mathematically">
<meta property="og:locale" content="en_US">
<meta name="description" content="Attention has gotten plenty of attention lately, after yielding state of the art results in multiple fields of research. From image captioning and language translation to interactive question answering, Attention has quickly become a key tool to which researchers must attend. Some have taken notice and even postulate that attention is all you need. But what is Attention anyway? Should you pay attention to Attention? Attention enables the model to focus in on important pieces of the feature space. In this post, we explain how the Attention mechanism works mathematically and then implement the equations using Keras. We conclude with discussing how to “see” the Attention mechanism at work by identifying important words for a classification task.">
<meta property="og:description" content="Attention has gotten plenty of attention lately, after yielding state of the art results in multiple fields of research. From image captioning and language translation to interactive question answering, Attention has quickly become a key tool to which researchers must attend. Some have taken notice and even postulate that attention is all you need. But what is Attention anyway? Should you pay attention to Attention? Attention enables the model to focus in on important pieces of the feature space. In this post, we explain how the Attention mechanism works mathematically and then implement the equations using Keras. We conclude with discussing how to “see” the Attention mechanism at work by identifying important words for a classification task.">
<link rel="canonical" href="http://srome.github.io//Understanding-Attention-in-Neural-Networks-Mathematically/">
<meta property="og:url" content="http://srome.github.io//Understanding-Attention-in-Neural-Networks-Mathematically/">
<meta property="og:site_name" content="Scott Rome">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-03-23T00:00:00+00:00">
<script async="" src="ref_25_files/analytics.js"></script><script async="" src="ref_25_files/analytics.download"></script><script type="application/ld+json">
{"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://srome.github.io//Understanding-Attention-in-Neural-Networks-Mathematically/"},"url":"http://srome.github.io//Understanding-Attention-in-Neural-Networks-Mathematically/","headline":"Understanding Attention in Neural Networks Mathematically","dateModified":"2018-03-23T00:00:00+00:00","datePublished":"2018-03-23T00:00:00+00:00","description":"Attention has gotten plenty of attention lately, after yielding state of the art results in multiple fields of research. From image captioning and language translation to interactive question answering, Attention has quickly become a key tool to which researchers must attend. Some have taken notice and even postulate that attention is all you need. But what is Attention anyway? Should you pay attention to Attention? Attention enables the model to focus in on important pieces of the feature space. In this post, we explain how the Attention mechanism works mathematically and then implement the equations using Keras. We conclude with discussing how to “see” the Attention mechanism at work by identifying important words for a classification task.","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>

  <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="https://srome.github.io/" class="site-avatar"><img src="ref_25_files/9755689"></a>

          <div class="site-info">
            <h1 class="site-name"><a href="https://srome.github.io/">Scott Rome</a></h1>
            <p class="site-description">Math Ph.D. who works in Machine Learning</p>
          </div>

          <nav>
            <a href="https://srome.github.io/">Home</a>
            <a href="https://srome.github.io/about">About</a>
            <a href="https://srome.github.io/archive#allposts">Archive</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <script type="text/javascript" src="ref_25_files/MathJax.download"></script>

<article class="post">
  <h1>Understanding Attention in Neural Networks Mathematically</h1>

  <div class="entry">
    <p>Attention has gotten plenty of attention lately, after yielding state of the art results in multiple fields of research. From <a href="https://arxiv.org/pdf/1502.03044.pdf">image captioning</a> and <a href="https://arxiv.org/pdf/1409.0473.pdf">language translation</a> to <a href="https://arxiv.org/pdf/1612.07411.pdf">interactive question answering</a>, Attention has quickly become a key tool to which researchers must attend. Some have taken notice and even postulate that <a href="https://arxiv.org/pdf/1706.03762.pdf">attention is all you need</a>.
 But what is Attention anyway? Should you pay attention to Attention? 
Attention enables the model to focus in on important pieces of the 
feature space. In this post, we explain how the Attention mechanism 
works mathematically and then implement the equations using Keras. We 
conclude with discussing how to “see” the Attention mechanism at work by
 identifying important words for a classification task.</p>

<p>Attention manifests differently in different contexts. In some NLP 
problems, Attention allows the model to put more emphasis on different 
words during prediction, e.g. from <a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf">Yang et. Al</a>:</p>

<p><img src="ref_25_files/attention.png" alt="png"></p>

<p>Interestingly, there is <a href="https://arxiv.org/pdf/1506.07285.pdf">increasing</a> <a href="https://arxiv.org/pdf/1503.08895.pdf">research</a>
 that calls this mechanism “Memory”, which some claim is a better title 
for such a versatile layer. Indeed, the Attention layer can allow a 
model to “look back” at previous examples that are relevant at 
prediction time, and the mechanism has been used in that way for so 
called Memory Networks. A discussion of Memory Networks is outside the 
scope of this post, so for our purposes, we will discuss Attention named
 as such, and explore the properties apparent to this nomenclature.</p>

<h2 id="the-attention-mechanism">The Attention Mechanism</h2>

<p>The Attention Mechanism mathematically is deceptively simple for its 
far reaching applications. I am going to go over the style of Attention 
that is common in NLP applications. A good overview of different 
approaches can be found <a href="http://ruder.io/deep-learning-nlp-best-practices/index.html#attention">here</a>. In this case, Attention can be broken down into a few key steps:</p>

<ol>
  <li>MLP: A one layer MLP acting on the hidden state of the word</li>
  <li>Word-level Context: A vector is dotted with the output of the MLP</li>
  <li>Softmax: The resulting vector is passed through a softmax layer</li>
  <li>Combination: The attention vector from the softmax is combined with the input state that was passed into the MLP</li>
</ol>

<p>Now we will describe this in equations. The general flavor of the notation is following the <a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf">paper</a> on Hierarchical Attention by Yang et. Al, but includes insight from the notation used in <a href="https://arxiv.org/pdf/1612.07411.pdf">this paper</a> by Li et. Al as well.</p>

<p>Let <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="msubsup" id="MathJax-Span-3"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-4" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="texatom" id="MathJax-Span-5"><span class="mrow" id="MathJax-Span-6"><span class="mi" id="MathJax-Span-7" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-1">h_{t}</script> be the word representation (either an embedding or a (concatenation) hidden state(s) of an RNN) of dimension <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-8" style="width: 1.762em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.438em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1001.44em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-9"><span class="msubsup" id="MathJax-Span-10"><span style="display: inline-block; position: relative; width: 1.438em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.88em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.836em;"><span class="mi" id="MathJax-Span-12" style="font-size: 70.7%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>K</mi><mi>w</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-2">K_w</script> for the word at position <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-13" style="width: 0.465em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.373em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.438em, 1000.33em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15" style="font-family: MathJax_Math-italic;">t</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">t</script> in some sentence padded to a length of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 1.299em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.067em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1001.07em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mi" id="MathJax-Span-18" style="font-family: MathJax_Math-italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.947em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-4">M</script>. Then we can describe the steps via</p>

<p><span class="MathJax_Preview" style="color: inherit; display: none;"></span></p><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;tanh&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 10.419em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.66em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.345em, 1008.57em, 2.595em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="msubsup" id="MathJax-Span-21"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-22" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-23" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-24" style="font-family: MathJax_Main; padding-left: 0.28em;">=</span><span class="mi" id="MathJax-Span-25" style="font-family: MathJax_Main; padding-left: 0.28em;">tanh</span><span class="mo" id="MathJax-Span-26"></span><span class="mo" id="MathJax-Span-27" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-28" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span class="msubsup" id="MathJax-Span-29"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-30" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-31" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-32" style="font-family: MathJax_Main; padding-left: 0.234em;">+</span><span class="mi" id="MathJax-Span-33" style="font-family: MathJax_Math-italic; padding-left: 0.234em;">b</span><span class="mo" id="MathJax-Span-34" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.331em; border-left: 0px solid; width: 0px; height: 1.336em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>u</mi><mi>t</mi></msub><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>W</mi><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-5">\begin{equation} u_t = \tanh(W h_t + b)\end{equation}</script>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;softmax&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-35" style="width: 10.188em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.475em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1008.38em, 2.595em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-36"><span class="msubsup" id="MathJax-Span-37"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-38" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.65em;"><span class="mi" id="MathJax-Span-39" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-40" style="font-family: MathJax_Main; padding-left: 0.28em;">=</span><span class="mtext" id="MathJax-Span-41" style="font-family: MathJax_Main; padding-left: 0.28em;">softmax</span><span class="mo" id="MathJax-Span-42" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-43"><span style="display: inline-block; position: relative; width: 1.067em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.47em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-44" style="font-family: MathJax_Math-italic;">v</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.465em;"><span class="mi" id="MathJax-Span-45" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-46"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-47" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-48" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-49" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.331em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><msup><mi>v</mi><mi>T</mi></msup><msub><mi>u</mi><mi>t</mi></msub><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-6">\begin{equation} \alpha_t = \text{softmax}(v^T u_t)\end{equation}</script>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-50" style="width: 6.391em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.326em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.373em, 1005.33em, 3.567em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-51"><span class="mi" id="MathJax-Span-52" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-53" style="font-family: MathJax_Main; padding-left: 0.28em;">=</span><span class="munderover" id="MathJax-Span-54" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 1.438em; height: 0px;"><span style="position: absolute; clip: rect(2.873em, 1001.39em, 4.586em, -999.998em); top: -3.979em; left: 0em;"><span class="mo" id="MathJax-Span-55" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.382em, 1001.11em, 4.215em, -999.998em); top: -2.914em; left: 0.141em;"><span class="texatom" id="MathJax-Span-56"><span class="mrow" id="MathJax-Span-57"><span class="mi" id="MathJax-Span-58" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-59" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-60" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.243em, 1000.74em, 4.123em, -999.998em); top: -5.137em; left: 0.373em;"><span class="texatom" id="MathJax-Span-61"><span class="mrow" id="MathJax-Span-62"><span class="mi" id="MathJax-Span-63" style="font-size: 70.7%; font-family: MathJax_Math-italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-64" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-65" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.65em;"><span class="mi" id="MathJax-Span-66" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-67"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-68" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-69" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 3.614em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>s</mi><mo>=</mo><munderover><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></munderover><msub><mi>α</mi><mi>t</mi></msub><msub><mi>h</mi><mi>t</mi></msub></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-7">\begin{equation} s = \sum_{t=1}^{M} \alpha_t h_t\end{equation}</script><p></p>

<p>If we define <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x00D7;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-70" style="width: 6.808em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.65em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1005.65em, 2.41em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-71"><span class="mi" id="MathJax-Span-72" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span class="mo" id="MathJax-Span-73" style="font-family: MathJax_Main; padding-left: 0.28em;">∈</span><span class="msubsup" id="MathJax-Span-74" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 3.382em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.7em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="texatom" id="MathJax-Span-75"><span class="mrow" id="MathJax-Span-76"><span class="mi" id="MathJax-Span-77" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.743em;"><span class="texatom" id="MathJax-Span-78"><span class="mrow" id="MathJax-Span-79"><span class="msubsup" id="MathJax-Span-80"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-81" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-82" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-83" style="font-size: 70.7%; font-family: MathJax_Main;">×</span><span class="msubsup" id="MathJax-Span-84"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-85" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-86" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.108em; border-left: 0px solid; width: 0px; height: 1.225em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mi>w</mi></msub><mo>×</mo><msub><mi>K</mi><mi>w</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-8">W\in \mathbb{R}^{K_w \times K_w}</script> and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-87" style="width: 5.28em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.4em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1004.4em, 2.549em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-88"><span class="mi" id="MathJax-Span-89" style="font-family: MathJax_Math-italic;">v</span><span class="mo" id="MathJax-Span-90" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-91" style="font-family: MathJax_Math-italic; padding-left: 0.188em;">b</span><span class="mo" id="MathJax-Span-92" style="font-family: MathJax_Main; padding-left: 0.28em;">∈</span><span class="msubsup" id="MathJax-Span-93" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 1.808em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.7em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="texatom" id="MathJax-Span-94"><span class="mrow" id="MathJax-Span-95"><span class="mi" id="MathJax-Span-96" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.743em;"><span class="texatom" id="MathJax-Span-97"><span class="mrow" id="MathJax-Span-98"><span class="msubsup" id="MathJax-Span-99"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-100" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-101" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.392em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi><mo>,</mo><mi>b</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mi>w</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-9">v,b\in\mathbb{R}^{K_w}</script>, then <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-102" style="width: 4.725em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.938em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1003.94em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-103"><span class="msubsup" id="MathJax-Span-104"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-105" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-106" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-107" style="font-family: MathJax_Main; padding-left: 0.28em;">∈</span><span class="msubsup" id="MathJax-Span-108" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 1.808em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.7em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="texatom" id="MathJax-Span-109"><span class="mrow" id="MathJax-Span-110"><span class="mi" id="MathJax-Span-111" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.743em;"><span class="texatom" id="MathJax-Span-112"><span class="mrow" id="MathJax-Span-113"><span class="msubsup" id="MathJax-Span-114"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-115" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-116" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.392em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>u</mi><mi>t</mi></msub><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mi>w</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-10">u_t \in \mathbb{R}^{K_w}</script>, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-117" style="width: 3.521em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.669em, 1002.92em, 2.78em, -999.998em); top: -2.498em; left: 0em;"><span class="mrow" id="MathJax-Span-118"><span class="msubsup" id="MathJax-Span-119"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-120" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.65em;"><span class="mi" id="MathJax-Span-121" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-122" style="font-family: MathJax_Main; padding-left: 0.28em;">∈</span><span class="texatom" id="MathJax-Span-123" style="padding-left: 0.28em;"><span class="mrow" id="MathJax-Span-124"><span class="mi" id="MathJax-Span-125" style="font-family: MathJax_AMS;">R</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.502em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>α</mi><mi>t</mi></msub><mo>∈</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-11">\alpha_t \in \mathbb{R}</script> and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-126" style="width: 4.215em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.521em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1003.52em, 2.41em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-127"><span class="mi" id="MathJax-Span-128" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-129" style="font-family: MathJax_Main; padding-left: 0.28em;">∈</span><span class="msubsup" id="MathJax-Span-130" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 1.808em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.7em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="texatom" id="MathJax-Span-131"><span class="mrow" id="MathJax-Span-132"><span class="mi" id="MathJax-Span-133" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.743em;"><span class="texatom" id="MathJax-Span-134"><span class="mrow" id="MathJax-Span-135"><span class="msubsup" id="MathJax-Span-136"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-137" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-138" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.108em; border-left: 0px solid; width: 0px; height: 1.225em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mi>w</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-12">s\in\mathbb{R}^{K_w}</script>.
 (Note: this is the multiplicative application of attention.) Then, the 
final option is to determine Even though there is a lot of notation, it 
is still three equations. How can models improve so much when using 
attention?</p>

<h3 id="how-attention-actually-works-geometrically">How Attention Actually Works (Geometrically)</h3>

<p>To answer that question, we’re going to explain what each step 
actually does geometrically, and this will illuminate names like 
“word-level context vector”. I recommend reading my <a href="http://srome.github.io/Visualizing-the-Learning-of-a-Neural-Network-Geometrically/">post</a> on how neural networks bend and twist the input space, so this next bit will make a lot more sense.</p>

<h4 id="the-single-layer-mlp">The Single Layer MLP</h4>

<p>First, remember that <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-139" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-140"><span class="msubsup" id="MathJax-Span-141"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-142" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-143" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-13">h_t</script>
 is an embedding or representation of a word in a vector space. That’s a
 fancy way of saying that we pick some N-dimensional space, and we pick a
 point for every word (hopefully in a clever way). For <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-144" style="width: 3.289em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.734em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1002.69em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-145"><span class="mi" id="MathJax-Span-146" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span class="mo" id="MathJax-Span-147" style="font-family: MathJax_Main; padding-left: 0.28em;">=</span><span class="mn" id="MathJax-Span-148" style="font-family: MathJax_Main; padding-left: 0.28em;">2</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.947em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mo>=</mo><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-14">N=2</script>,
 we would be drawing a bunch of dots on a piece of paper to represent 
our words. Every layer of the neural net moves these dots around (and 
transforms the paper itself!), and this is the key to understanding how 
attention works. The first piece</p>

<p><span class="MathJax_Preview" style="color: inherit; display: none;"></span></p><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-149" style="width: 10.465em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.706em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.345em, 1008.61em, 2.595em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-150"><span class="msubsup" id="MathJax-Span-151"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-152" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-153" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-154" style="font-family: MathJax_Main; padding-left: 0.28em;">=</span><span class="mi" id="MathJax-Span-155" style="font-family: MathJax_Math-italic; padding-left: 0.28em;">t</span><span class="mi" id="MathJax-Span-156" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-157" style="font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-158" style="font-family: MathJax_Math-italic;">h</span><span class="mo" id="MathJax-Span-159" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-160" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span class="msubsup" id="MathJax-Span-161"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-162" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-163" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-164" style="font-family: MathJax_Main; padding-left: 0.234em;">+</span><span class="mi" id="MathJax-Span-165" style="font-family: MathJax_Math-italic; padding-left: 0.234em;">b</span><span class="mo" id="MathJax-Span-166" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.331em; border-left: 0px solid; width: 0px; height: 1.336em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>u</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-15">\begin{equation}u_t = tanh(Wh_t+b)\end{equation}</script><p></p>

<p>takes the word, rotates/scales it and then translates it. So, this 
layer rearranges the word representation in its current vector space. 
The tanh activation then twists and bends (but does not break!) the 
vector space into a manifold. Because <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;&amp;#x2192;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-167" style="width: 8.475em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.039em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1007.04em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-168"><span class="mi" id="MathJax-Span-169" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span class="mo" id="MathJax-Span-170" style="font-family: MathJax_Main; padding-left: 0.28em;">:</span><span class="msubsup" id="MathJax-Span-171" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 1.808em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.7em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="texatom" id="MathJax-Span-172"><span class="mrow" id="MathJax-Span-173"><span class="mi" id="MathJax-Span-174" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.743em;"><span class="texatom" id="MathJax-Span-175"><span class="mrow" id="MathJax-Span-176"><span class="msubsup" id="MathJax-Span-177"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-178" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-179" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-180" style="font-family: MathJax_Main; padding-left: 0.28em;">→</span><span class="msubsup" id="MathJax-Span-181" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 1.808em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.7em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="texatom" id="MathJax-Span-182"><span class="mrow" id="MathJax-Span-183"><span class="mi" id="MathJax-Span-184" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.743em;"><span class="texatom" id="MathJax-Span-185"><span class="mrow" id="MathJax-Span-186"><span class="msubsup" id="MathJax-Span-187"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-188" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-189" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 1.225em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi><mo>:</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mi>w</mi></msub></mrow></msup><mo stretchy="false">→</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mi>w</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-16">W:\mathbb{R}^{K_w}\to\mathbb{R}^{K_w}</script> (i.e., it does not change the dimension of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-190" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-191"><span class="msubsup" id="MathJax-Span-192"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-193" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-194" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-17">h_t</script>), then there is no information lost in this step. If <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-195" style="width: 1.252em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1001.02em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-196"><span class="mi" id="MathJax-Span-197" style="font-family: MathJax_Math-italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.947em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-18">W</script> projected <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-19-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-198" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-199"><span class="msubsup" id="MathJax-Span-200"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-201" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-202" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-19">h_t</script> to a lower dimensional space, then we would lose information from our word embedding. Embedding <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-20-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-203" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-204"><span class="msubsup" id="MathJax-Span-205"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-206" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-207" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-20">h_t</script> into a higher dimensional space would not give us any new information about <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-21-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-208" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-209"><span class="msubsup" id="MathJax-Span-210"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-211" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-212" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-21">h_t</script>, but it would add computational complexity, so no one does that.</p>

<h4 id="the-word-level-context-vector">The Word-Level Context Vector</h4>

<p>The word-level context vector is then applied via a dot product:
<span class="MathJax_Preview" style="color: inherit; display: none;"></span></p><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-22-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msup&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="text-align: center; position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-213" style="width: 2.363em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.947em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1001.95em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-214"><span class="msubsup" id="MathJax-Span-215"><span style="display: inline-block; position: relative; width: 1.067em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.47em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-216" style="font-family: MathJax_Math-italic;">v</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.465em;"><span class="mi" id="MathJax-Span-217" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-218"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-219" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-220" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.392em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>v</mi><mi>T</mi></msup><msub><mi>u</mi><mi>t</mi></msub></math></span></span></div><script type="math/tex; mode=display" id="MathJax-Element-22">\begin{equation}v^T u_t\end{equation}</script>
As both <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-23-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2208;&lt;/mo&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-221" style="width: 5.836em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.863em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.206em, 1004.86em, 2.549em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-222"><span class="mi" id="MathJax-Span-223" style="font-family: MathJax_Math-italic;">v</span><span class="mo" id="MathJax-Span-224" style="font-family: MathJax_Main;">,</span><span class="msubsup" id="MathJax-Span-225" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-226" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-227" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="mo" id="MathJax-Span-228" style="font-family: MathJax_Main; padding-left: 0.28em;">∈</span><span class="msubsup" id="MathJax-Span-229" style="padding-left: 0.28em;"><span style="display: inline-block; position: relative; width: 1.808em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.7em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="texatom" id="MathJax-Span-230"><span class="mrow" id="MathJax-Span-231"><span class="mi" id="MathJax-Span-232" style="font-family: MathJax_AMS;">R</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.396em; left: 0.743em;"><span class="texatom" id="MathJax-Span-233"><span class="mrow" id="MathJax-Span-234"><span class="msubsup" id="MathJax-Span-235"><span style="display: inline-block; position: relative; width: 1.021em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.65em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-236" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.887em; left: 0.604em;"><span class="mi" id="MathJax-Span-237" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.392em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi><mo>,</mo><msub><mi>u</mi><mi>t</mi></msub><mo>∈</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>K</mi><mi>w</mi></msub></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-23">v,u_t\in \mathbb{R}^{K_w}</script>, it is <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-24-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-238" style="width: 0.558em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.47em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-239"><span class="mi" id="MathJax-Span-240" style="font-family: MathJax_Math-italic;">v</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi></math></span></span><script type="math/tex" id="MathJax-Element-24">v</script>’s job to learn information of this new vector space induced by the previous layer. You can think of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-25-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-241" style="width: 0.558em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.47em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-242"><span class="mi" id="MathJax-Span-243" style="font-family: MathJax_Math-italic;">v</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi></math></span></span><script type="math/tex" id="MathJax-Element-25">v</script> as combining the components of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-26-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-244" style="width: 1.067em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.88em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-245"><span class="msubsup" id="MathJax-Span-246"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-247" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-248" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 0.836em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>u</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-26">u_t</script> according to their relevance to the problem at hand. Geometrically, you can imagine that <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-27-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-249" style="width: 0.558em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.47em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-250"><span class="mi" id="MathJax-Span-251" style="font-family: MathJax_Math-italic;">v</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi></math></span></span><script type="math/tex" id="MathJax-Element-27">v</script> “emphasizes” the important dimensions of the vector space.<p></p>

<p>Let’s see an example. Say we are working on a classification task: is
 this word an animal or not. Imagine you have a word embedding defined 
by the following image:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="n">vectors</span> <span class="o">=</span> <span class="p">{</span><span class="s">'dog'</span> <span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">],</span> <span class="s">'cat'</span><span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="o">.</span><span class="mi">48</span><span class="p">],</span> <span class="s">'snow'</span> <span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="o">.</span><span class="mo">02</span><span class="p">],</span> <span class="s">'rain'</span> <span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="o">.</span><span class="mo">05</span><span class="p">]</span> <span class="p">}</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">item</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="ref_25_files/output_1_0.png" alt="png"></p>

<p>In this toy example, we can assume that the x-axis component of this 
word embedding is NOT useful for the task of classifying animals, if 
this was all of our data. So, a choice of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-28-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-252" style="width: 0.558em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.47em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-253"><span class="mi" id="MathJax-Span-254" style="font-family: MathJax_Math-italic;">v</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi></math></span></span><script type="math/tex" id="MathJax-Element-28">v</script> might be the vector <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-29-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-255" style="width: 2.456em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.039em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.345em, 1001.9em, 2.595em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-256"><span class="mo" id="MathJax-Span-257" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-258" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-259" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-260" style="font-family: MathJax_Main; padding-left: 0.188em;">1</span><span class="mo" id="MathJax-Span-261" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.331em; border-left: 0px solid; width: 0px; height: 1.336em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-29">[0,1]</script>, which would pluck out the <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-30-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-262" style="width: 0.604em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.512em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.51em, 2.549em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-263"><span class="mi" id="MathJax-Span-264" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></span></span><script type="math/tex" id="MathJax-Element-30">y</script> dimension, as it is clearly important, and remove the <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-31-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-265" style="width: 0.697em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.558em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.51em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-266"><span class="mi" id="MathJax-Span-267" style="font-family: MathJax_Math-italic;">x</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-31">x</script>. The resulting clustering would look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Here we apply v, which zeros out the first dimension</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="ref_25_files/output_3_0.png" alt="png"></p>

<p>We can clearly see that <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-32-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-268" style="width: 0.558em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.47em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-269"><span class="mi" id="MathJax-Span-270" style="font-family: MathJax_Math-italic;">v</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi></math></span></span><script type="math/tex" id="MathJax-Element-32">v</script>
 has identified important information in the vector space representing 
the words. This is what the word-level context vector’s role is. It 
picks out the important parts of the vector space for our model to focus
 in on. This means that the previous layer will move the input vector 
space around so that <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-33-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-271" style="width: 0.558em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.47em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-272"><span class="mi" id="MathJax-Span-273" style="font-family: MathJax_Math-italic;">v</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi></math></span></span><script type="math/tex" id="MathJax-Element-33">v</script>
 can easily pick out the important words! In fact, the softmax values of
 “dog” and “cat” will be larger in this case than “rain” and “snow”. 
That’s exactly what we want.</p>

<p>The next layer does not have a particularly geometric flavor to it. 
The application of the softmax is a monotonic transformation (i.e. the 
importances picked out by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-34-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-274" style="width: 2.363em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.947em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.252em, 1001.95em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-275"><span class="msubsup" id="MathJax-Span-276"><span style="display: inline-block; position: relative; width: 1.067em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.47em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-277" style="font-family: MathJax_Math-italic;">v</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -4.35em; left: 0.465em;"><span class="mi" id="MathJax-Span-278" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.095em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-279"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-280" style="font-family: MathJax_Math-italic;">u</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-281" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.336em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>v</mi><mi>T</mi></msup><msub><mi>u</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-34">v^Tu_t</script> stay the same). This allows the final representation of the input sentence to be a linear combination of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-35-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-282" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-283"><span class="msubsup" id="MathJax-Span-284"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-285" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-286" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-35">h_t</script>, weighting the word representations by their importance. There is however another way to view the Attention output…</p>

<h3 id="probabilistic-interpretation-of-the-output">Probabilistic Interpretation of the Output</h3>

<p>The output is easy to understand probabilistically. Because <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-36-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;msubsup&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/msubsup&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-287" style="width: 5.743em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.771em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.252em, 1004.77em, 2.688em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-288"><span class="mi" id="MathJax-Span-289" style="font-family: MathJax_Math-italic;">α</span><span class="mo" id="MathJax-Span-290" style="font-family: MathJax_Main; padding-left: 0.28em;">=</span><span class="mo" id="MathJax-Span-291" style="font-family: MathJax_Main; padding-left: 0.28em;">[</span><span class="msubsup" id="MathJax-Span-292"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-293" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.65em;"><span class="mi" id="MathJax-Span-294" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-295"><span style="display: inline-block; position: relative; width: 1.53em; height: 0px;"><span style="position: absolute; clip: rect(3.104em, 1000.14em, 4.354em, -999.998em); top: -3.979em; left: 0em;"><span class="mo" id="MathJax-Span-296" style="font-family: MathJax_Main;">]</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.382em, 1000.84em, 4.123em, -999.998em); top: -4.303em; left: 0.28em;"><span class="mi" id="MathJax-Span-297" style="font-size: 70.7%; font-family: MathJax_Math-italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; clip: rect(3.382em, 1001.25em, 4.123em, -999.998em); top: -3.655em; left: 0.28em;"><span class="texatom" id="MathJax-Span-298"><span class="mrow" id="MathJax-Span-299"><span class="mi" id="MathJax-Span-300" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-301" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-302" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.442em; border-left: 0px solid; width: 0px; height: 1.503em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>α</mi><mi>t</mi></msub><msubsup><mo stretchy="false">]</mo><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup></math></span></span><script type="math/tex" id="MathJax-Element-36">\alpha=[\alpha_t]_{t=1}^M</script> sums to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-37-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-303" style="width: 0.604em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.512em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.438em, 1000.42em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-304"><span class="mn" id="MathJax-Span-305" style="font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-37">1</script>, this yields a probabilistic interpretation of Attention. Each weight <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-38-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-306" style="width: 1.16em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.98em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-307"><span class="msubsup" id="MathJax-Span-308"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-309" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.65em;"><span class="mi" id="MathJax-Span-310" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 0.836em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>α</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-38">\alpha_t</script>
 indicates the probability that the word is important to the problem at 
hand. There are some versions of attention (“hard attention”) which 
actually will sample a word according to this probability only return 
that word from the attention layer. In our case, the resulting vector is
 the expectation of the important words. In particular, if our random 
variable <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-39-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-311" style="width: 1.067em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.882em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.88em, 2.363em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-312"><span class="mi" id="MathJax-Span-313" style="font-family: MathJax_Math-italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.947em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></span></span><script type="math/tex" id="MathJax-Element-39">X</script> is defined to take on the value <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-40-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-314" style="width: 1.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.391em, 1000.93em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-315"><span class="msubsup" id="MathJax-Span-316"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.15em, 1000.56em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-317" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.558em;"><span class="mi" id="MathJax-Span-318" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 1.114em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-40">h_t</script> with probability <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-41-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-319" style="width: 1.16em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1000.98em, 2.502em, -999.998em); top: -2.22em; left: 0em;"><span class="mrow" id="MathJax-Span-320"><span class="msubsup" id="MathJax-Span-321"><span style="display: inline-block; position: relative; width: 0.975em; height: 0px;"><span style="position: absolute; clip: rect(3.382em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;"><span class="mi" id="MathJax-Span-322" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.65em;"><span class="mi" id="MathJax-Span-323" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 3.984em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.225em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.219em; border-left: 0px solid; width: 0px; height: 0.836em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>α</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-41">\alpha_t</script>, then the output of our Attention mechanism is actually <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-42-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-324" style="width: 2.502em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.623em, 1001.95em, 2.873em, -999.998em); top: -2.498em; left: 0em;"><span class="mrow" id="MathJax-Span-325"><span class="texatom" id="MathJax-Span-326"><span class="mrow" id="MathJax-Span-327"><span class="mi" id="MathJax-Span-328" style="font-family: MathJax_AMS;">E</span></span></span><span class="mo" id="MathJax-Span-329" style="font-family: MathJax_Main;">[</span><span class="mi" id="MathJax-Span-330" style="font-family: MathJax_Math-italic;">X<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.049em;"></span></span><span class="mo" id="MathJax-Span-331" style="font-family: MathJax_Main;">]</span></span><span style="display: inline-block; width: 0px; height: 2.502em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.331em; border-left: 0px solid; width: 0px; height: 1.336em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow><mo stretchy="false">[</mo><mi>X</mi><mo stretchy="false">]</mo></math></span></span><script type="math/tex" id="MathJax-Element-42">\mathbb{E}[X]</script>.</p>

<h1 id="the-data">The Data</h1>

<p>To demonstrate this, we’re going to use data from a recent <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/">Kaggle competition</a>
 on using NLP to detect toxic comments. The competition aims to label a 
text with 6 different (somewhat correlated) labels: “toxic”, 
“severe_toxic”, “obscene”, “threat”, “insult”, and “identity_hate”. We 
will expect the Attention layer to give more importance to negative 
words that could indicate one of those labels have occured. Let’s do a 
quick data load with some boilerplate code…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'train.csv'</span><span class="p">)</span>
<span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">list_classes</span> <span class="o">=</span> <span class="p">[</span><span class="s">"toxic"</span><span class="p">,</span> <span class="s">"severe_toxic"</span><span class="p">,</span> <span class="s">"obscene"</span><span class="p">,</span> <span class="s">"threat"</span><span class="p">,</span> <span class="s">"insult"</span><span class="p">,</span> <span class="s">"identity_hate"</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">list_classes</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example of a data point</span>
<span class="n">train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">159552</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>id                                                07ea42c192ead7d9
comment_text     "\n\nafd\nHey, MONGO, why dont you put a proce...
toxic                                                            0
severe_toxic                                                     0
obscene                                                          0
threat                                                           0
insult                                                           0
identity_hate                                                    0
Name: 2928, dtype: object
</code></pre></div></div>

<p>More NLP boilerplate to tokenize and pad the sequences:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_features</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">maxlen</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">list_sentences</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s">'comment_text'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c"># Tokenize</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">list_sentences</span><span class="p">))</span>
<span class="n">list_tokenized_train</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">list_sentences</span><span class="p">)</span>

<span class="c"># Pad</span>
<span class="n">X_t</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">list_tokenized_train</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="keras-architecture">Keras Architecture</h2>

<p>There are many <a href="https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2">versions</a>
 of attention out there that actually implements a custom Keras layer 
and does the calculations with low-level calls to the Keras backend. In 
my implementation, I’d like to avoid this and instead use Keras layers 
to build up the Attention layer in an attempt to demystify what is going
 on.</p>

<h4 id="attention-implementation">Attention Implementation</h4>

<p>For our implementation, we’re going to follow the formulas used above. The approach was inspired by <a href="https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention_lstm.py">this repository</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Concatenate</span><span class="p">,</span> <span class="n">Permute</span><span class="p">,</span> <span class="n">SpatialDropout1D</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Bidirectional</span><span class="p">,</span> <span class="n">Multiply</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span><span class="n">Flatten</span><span class="p">,</span><span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">keras.backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="k">class</span> <span class="nc">Attention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">combine</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c"># Expects inp to be of size (?, number of words, embedding dimension)</span>
        
        <span class="n">repeat_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="c"># Map through 1 Layer MLP</span>
        <span class="n">x_a</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">,</span> <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s">'glorot_uniform'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"tanh"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"tanh_mlp"</span><span class="p">)(</span><span class="n">inp</span><span class="p">)</span> 
        
        <span class="c"># Dot with word-level vector</span>
        <span class="n">x_a</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s">'glorot_uniform'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"word-level_context"</span><span class="p">)(</span><span class="n">x_a</span><span class="p">)</span>
        <span class="n">x_a</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x_a</span><span class="p">)</span> <span class="c"># x_a is of shape (?,200,1), we flatten it to be (?,200)</span>
        <span class="n">att_out</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">)(</span><span class="n">x_a</span><span class="p">)</span> 
        
        <span class="c"># Clever trick to do elementwise multiplication of alpha_t with the correct h_t:</span>
        <span class="c"># RepeatVector will blow it out to be (?,120, 200)</span>
        <span class="c"># Then, Permute will swap it to (?,200,120) where each row (?,k,120) is a copy of a_t[k]</span>
        <span class="c"># Then, Multiply performs elementwise multiplication to apply the same a_t to each</span>
        <span class="c"># dimension of the respective word vector</span>
        <span class="n">x_a2</span> <span class="o">=</span> <span class="n">RepeatVector</span><span class="p">(</span><span class="n">repeat_size</span><span class="p">)(</span><span class="n">att_out</span><span class="p">)</span>
        <span class="n">x_a2</span> <span class="o">=</span> <span class="n">Permute</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])(</span><span class="n">x_a2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">inp</span><span class="p">,</span><span class="n">x_a2</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">combine</span><span class="p">:</span>
        <span class="c"># Now we sum over the resulting word representations</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'expectation_over_words'</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">att_out</span><span class="p">)</span>
                   
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<h3 id="model-architecture">Model Architecture</h3>

<p>We are going to train a Bi-Directional LSTM to demonstrate the 
Attention class. The Bidirectional class in Keras returns a tensor with 
the same number of time steps as the input tensor, but with the forward 
and backward pass of the LSTM concatenated. Just to remind the reader, 
the standard dimensions for this use case in Keras is: (batch size, time
 steps, word embedding dimension).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm_shape</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c"># Define the model</span>
<span class="n">inp</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,))</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">input_length</span> <span class="o">=</span> <span class="n">maxlen</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">embed_size</span><span class="p">)(</span><span class="n">inp</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">SpatialDropout1D</span><span class="p">(</span><span class="mf">0.35</span><span class="p">)(</span><span class="n">emb</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">lstm_shape</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.15</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>

<span class="n">attention_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">attention</span><span class="p">)</span> <span class="c"># Model to print out the attention data</span>
</code></pre></div></div>

<p>If we look at the summary, we can trace the dimensions through the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 200)          0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 200, 128)     2560000     input_31[0][0]                   
__________________________________________________________________________________________________
spatial_dropout1d_31 (SpatialDr (None, 200, 128)     0           embedding_31[0][0]               
__________________________________________________________________________________________________
bidirectional_37 (Bidirectional (None, 200, 120)     90720       spatial_dropout1d_31[0][0]       
__________________________________________________________________________________________________
tanh_mlp (Dense)                (None, 200, 120)     14520       bidirectional_37[0][0]           
__________________________________________________________________________________________________
word-level_context (Dense)      (None, 200, 1)       121         tanh_mlp[0][0]                   
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 200)          0           word-level_context[0][0]         
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 200)          0           flatten_30[0][0]                 
__________________________________________________________________________________________________
repeat_vector_29 (RepeatVector) (None, 120, 200)     0           activation_11[0][0]              
__________________________________________________________________________________________________
permute_31 (Permute)            (None, 200, 120)     0           repeat_vector_29[0][0]           
__________________________________________________________________________________________________
multiply_28 (Multiply)          (None, 200, 120)     0           bidirectional_37[0][0]           
                                                                 permute_31[0][0]                 
__________________________________________________________________________________________________
expectation_over_words (Lambda) (None, 120)          0           multiply_28[0][0]                
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 6)            726         expectation_over_words[0][0]     
==================================================================================================
Total params: 2,666,087
Trainable params: 2,666,087
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre></div></div>

<h3 id="training">Training</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 127656 samples, validate on 31915 samples
Epoch 1/3
127656/127656 [==============================] - 192s 2ms/step - loss: 0.1686 - acc: 0.9613 - val_loss: 0.1388 - val_acc: 0.9639
Epoch 2/3
127656/127656 [==============================] - 183s 1ms/step - loss: 0.0939 - acc: 0.9708 - val_loss: 0.0526 - val_acc: 0.9814
Epoch 3/3
127656/127656 [==============================] - 183s 1ms/step - loss: 0.0492 - acc: 0.9820 - val_loss: 0.0480 - val_acc: 0.9828





&lt;keras.callbacks.History at 0x7f5c60dbc690&gt;
</code></pre></div></div>

<h2 id="identifying-the-words-the-model-attends">Identifying The Words the Model Attends</h2>

<p>Because we stored the output of the softmax layer, we can identify 
what words the model identified as important for classifying each 
sentence. We’ll define a helper function that will print the words in 
the sentence and pair them with their corresponding Attention output 
(i.e., the word’s importance to the prediction).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_word_importances</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">lt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">([</span><span class="n">text</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">lt</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">att</span> <span class="o">=</span> <span class="n">attention_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span><span class="p">,</span> <span class="p">[(</span><span class="n">reverse_token_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">importance</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">importance</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">att</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">reverse_token_map</span><span class="p">]</span>
</code></pre></div></div>

<p>The data set from Kaggle is about classifying toxic comments. Instead
 of using examples from the data (which is filled with vulgarities…), 
I’ll use a relatively benign example to show Attention in action:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This example is not offensive, according to the model.</span>
<span class="n">get_word_importances</span><span class="p">(</span><span class="s">'ice cream'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([[0.03289272, 0.00060764, 0.00365122, 0.00135027, 0.00818442,
         0.00286318]], dtype=float32),
 [('ice', 0.24796312), ('cream', 0.18920079)])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This example is labeled ALMOST labeled toxic, but isn't, due to the word "worthless".</span>
<span class="n">get_word_importances</span><span class="p">(</span><span class="s">'ice cream is worthless'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([[0.4471274 , 0.00662996, 0.07287972, 0.00910207, 0.09902474,
         0.02583557]], dtype=float32),
 [('ice', 0.028298836),
  ('cream', 0.024065288),
  ('is', 0.041414138),
  ('worthless', 0.8417008)])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This example is labeled toxic and it's because of the word "sucks".</span>
<span class="n">get_word_importances</span><span class="p">(</span><span class="s">'ice cream sucks'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([[0.9310674 , 0.04415609, 0.64935035, 0.02346741, 0.4773681 ,
         0.07371927]], dtype=float32),
 [('ice', 0.0032987772), ('cream', 0.003724447), ('sucks', 0.9874626)])
</code></pre></div></div>

<h1 id="discussion">Discussion</h1>

<p>Attention mechanisms are an exciting technique and an active area of 
research with new approaches being published all the time. Attention has
 been shown to help achieve state of the art performance on many tasks, 
but it is also has the obvious advantage of explaining important 
components of neural networks to a practitioner. In an academic setting,
 Attention proves very useful for model debugging by allowing one to 
inspect the predictions and their reasoning. Moreover, having the option
 available to indicate why a model made a prediction could be very 
important to fields like healthcare where predictions may be vetted by 
an expert.</p>

  </div>

  <div class="date">
    Written on March 23, 2018
  </div>

<!---
The following code is shamelessly taken from
https://blog.webjeda.com/jekyll-related-posts/-->

<div class="relatedPosts">

<h4>You May Also Enjoy</h4> 





  

    
    

    

    

  

    
    

    

    
      <div>
      <h5><a href="https://srome.github.io/Using-Ordinary-Differential-Equations-To-Design-State-of-the-Art-Residual-Style-Layers/">Using Ordinary Differential Equations To Design State of the Art Residual-Style Layers <span class="label label-default">Machine Learning</span>  <span class="label label-default">Theory</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="https://srome.github.io/Learning-About-Deep-Reinforcement-Learning-(Slides)/">Learning About Deep Reinforcement Learning (Slides) <span class="label label-default">Machine Learning</span>  <span class="label label-default">Theory</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    

  

    
    

    

    
      <div>
      <h5><a href="https://srome.github.io/Adversarial-Dreaming-with-TensorFlow-and-Keras/">Adversarial Dreaming with TensorFlow and Keras <span class="label label-default">Machine Learning</span>  <span class="label label-default">Theory</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="https://srome.github.io/Async-SGD-in-Python-Implementing-Hogwild!/">Hogwild!? Implementing Async SGD in Python <span class="label label-default">Machine Learning</span>  <span class="label label-default">Theory</span> </a></h5>
      </div>
      
      
        

</div>
  
</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:romescott@gmail.com"><i class="svg-icon email"></i></a>


<a href="https://github.com/srome"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/scottarome"><i class="svg-icon linkedin"></i></a>

<a href="https://srome.github.io/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/_srome"><i class="svg-icon twitter"></i></a>



        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-75065205-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/Understanding-Attention-in-Neural-Networks-Mathematically/',
		  'title': 'Understanding Attention in Neural Networks Mathematically'
		});
	</script>
	<!-- End Google Analytics -->


  

<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>